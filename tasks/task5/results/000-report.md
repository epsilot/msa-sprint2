# Task 5

- Для разворачивания сервиса единожды через helm создан отдельный chart booking-service-common.
- Рассматривались альтернативные варианты. Например, деплой через kubectl, но было желание использовать одну продвинутую
  технологию. Также был вариант оставить единый chart, но при этом создание сервиса сделать условным, например,
  по наличию какой-либо переменной, однако такое решение показалось неэлегантным и подверженным ошибкам в случае
  изменения переменных в values.
- Трафик между разными версиями делится при помощи DestinationRule subsets.
- Отказоустойчивость обеспечивается за счёт установки ретраев, ограничений на количество запросов (как на L4,
  так и на L7), а также отслеживание аутлаеров и временного исключения проблемных инстансов.
- Для удобства тестов используется единый докер образ, в который передаётся переменная окружения с версией.
- Для маршрутизации внешнего трафика внутрь кластера конфигурируется istio Gateway, который позволяет принмать трафик с
  localhost.
- Маршрутизация по средствам заголовка реализована в envoy-filter и вносит правки в istio ingressgateway.
- check-canary.sh модифицирован для наглядности тестирования. Отправка запросов идёт на endpoint index, чтобы было понятно
  к какой версии сервиса был маршрутизирован трафик.
- check-feature-flag.sh модифицирован для наглядности тестирования. Сначала идёт отправка нескольких запросов без
  заголовка, ответственного за маршрутизацию, чтобы убедиться, что маршрутизация идёт согласно весам (90/10). Затем
  отправляется пул запросов со специальным заголовком, чтобы убедиться, что все запросы направлены на сервис v2.